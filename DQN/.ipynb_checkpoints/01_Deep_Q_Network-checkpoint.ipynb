{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596963ce",
   "metadata": {},
   "source": [
    "# Deep Q Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a99e1a",
   "metadata": {},
   "source": [
    "What we need are:\n",
    "1. $Q_\\theta(s,a)$ to tell how good it is to take action $a$ given state $s$, parameterized by $\\theta$\n",
    "2. A policy $\\pi_\\theta(s)$ utilize the $Q_\\theta(s,a)$, which is $\\pi_\\theta(s)=\\arg\\max_a Q_\\theta(s,a)$, but also be able to explore different actions to find better one.\n",
    "3. Temporal difference $\\delta(s_t, a_t, s_{t+1}, r_{t+1})=Q_\\theta(s_t, a_t) - (r_{t+1} + \\max_{a_{t+1}}\\gamma Q_\\theta(s_{t+1}, a_{t+1}))$\n",
    "4. A replay buffer to store tranistion $(s_t, a_t, s_{t+1}, r_{t+1})$ to improve sample efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9dc08",
   "metadata": {},
   "source": [
    "### Define the Deep Q Network\n",
    "What we need is $Q_\\theta:S\\times A\\rightarrow R$.\n",
    "\n",
    "For a discrete action space, we can instead define a function of type $S\\rightarrow R^{|A|}$. We also need $V_\\theta:S\\rightarrow R$ defined as $V_\\theta(s) =\\max_a Q_\\theta(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c307f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, dim_state, n_actions):\n",
    "        super().__init__()\n",
    "        self.dim_state = dim_state\n",
    "        self.n_actions = n_actions\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim_state, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_actions)\n",
    "        )\n",
    "        self.forward = self.net.forward\n",
    "    \n",
    "    def q_vals(self, states, actions):\n",
    "        #Q(s,a)\n",
    "        return self.forward(states).gather(\n",
    "            1, actions.reshape(-1, 1).to(torch.int64)\n",
    "        ).reshape(-1)\n",
    "    \n",
    "    def v_vals(self, states):\n",
    "        #max_a Q(s,a)\n",
    "        return self.forward(states).amax(-1)\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.net.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.net.load_state_dict(torch.load(path))\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.net.parameters()).device\n",
    "\n",
    "    def get_compat(self, x):\n",
    "        x = np.stack(x)\n",
    "        x = torch.tensor(x, device=self.device)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6742109",
   "metadata": {},
   "source": [
    "### Define the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c7a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *arg):\n",
    "        self.deque.append(Transition(*arg))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.deque)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a3f99",
   "metadata": {},
   "source": [
    "### Define the Policy\n",
    "Use $\\epsilon$-greedy as exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0074f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class EpsGreedy:\n",
    "    def __init__(self, q_net):\n",
    "        self.q_value_net = q_net\n",
    "    \n",
    "    def get_action(self, state, mode=\"eval\", eps=0.1):\n",
    "        if mode == \"train\" and random.random() < eps:\n",
    "            return random.randint(0, self.q_value_net.n_actions-1)\n",
    "        else:\n",
    "            state = self.q_value_net.get_compat(state)\n",
    "            return self.q_value_net(state).argmax().item()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b5ec1",
   "metadata": {},
   "source": [
    "### Training\n",
    "To stablize training, use a target net, parametrized by $\\theta'$, to give value of the next state $s_{t+1}$.\n",
    "\n",
    "$\\delta(s_t, a_t, s_{t+1}, r_{t+1})=Q_\\theta(s_t, a_t) - (r_{t+1} + \\max_{a_{t+1}}\\gamma Q_{\\theta'}(s_{t+1}, a_{t+1}))$\n",
    "\n",
    "$\\theta'$ would be synchronized with $\\theta$ every a certain number of update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23037ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from itertools import count\n",
    "from IPython.display import display\n",
    "\n",
    "class DQNTrainer:\n",
    "    def __init__(self, q_net, env,\n",
    "                 lr=1e-4, gamma=0.99, replay_buf_size=10000,\n",
    "                 eps_start=0.5, eps_end=0.01, eps_decay=1e4,\n",
    "                 update_thres=500):\n",
    "        \n",
    "        self.q_net = q_net\n",
    "        self.env = env\n",
    "        self.opt = torch.optim.RMSprop(\n",
    "            self.q_net.parameters(),\n",
    "            lr=lr\n",
    "        )\n",
    "        self.gamma=gamma\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.policy = EpsGreedy(q_net)\n",
    "        self.replay_buf = ReplayBuffer(replay_buf_size)\n",
    "        self.target_net = copy.deepcopy(self.q_net)\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        \n",
    "        self.update_thres=update_thres\n",
    "        \n",
    "    @property\n",
    "    def eps(self):\n",
    "        ratio = np.exp(-self.counter/self.eps_decay)\n",
    "        eps = self.eps_end + (self.eps_start - self.eps_end)*ratio\n",
    "        return eps\n",
    "           \n",
    "    def update(self, batch_size):\n",
    "        if batch_size > len(self.replay_buf):\n",
    "            return\n",
    "        batch = self.replay_buf.sample(batch_size)\n",
    "        #check next_state to tell if is final\n",
    "        non_final_mask = [*map(lambda trans: trans.next_state is not None, batch)]\n",
    "        state, action, next_state, reward = zip(*batch)\n",
    "        next_state = [*filter(lambda x: x is not None, next_state)]\n",
    "\n",
    "        batch = Transition(*map(self.q_net.get_compat , [state, action, next_state, reward]))\n",
    "        #Q^(s,a)\n",
    "        state_action_vals = self.q_net.q_vals(batch.state, batch.action)\n",
    "        #V(s')\n",
    "        next_state_vals = torch.zeros_like(state_action_vals)\n",
    "        with torch.no_grad():\n",
    "            next_state_vals[non_final_mask] = self.target_net.v_vals(batch.next_state)\n",
    "        #Q(s,a)=r + gamma*V(s')\n",
    "        expected_state_action_vals = batch.reward + self.gamma * next_state_vals\n",
    "        \n",
    "        #compute loss & update\n",
    "        loss = self.criterion(state_action_vals, expected_state_action_vals)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        for para in self.q_net.parameters():\n",
    "            para.grad.data.clamp_(-1, 1)\n",
    "        self.opt.step()\n",
    "    \n",
    "    def train(self, n_episodes):\n",
    "        for i in range(n_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            for _ in count():\n",
    "                self.counter += 1\n",
    "                action = self.policy.get_action(state, \"train\", self.eps)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                \n",
    "                total_reward += reward\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                \n",
    "                self.replay_buf.push(state, action, next_state, reward)\n",
    "                \n",
    "                state = next_state\n",
    "                self.update(512)\n",
    "                if self.counter % self.update_thres == 0:\n",
    "                    self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "                \n",
    "                if terminated or truncated:break\n",
    "                    \n",
    "            if i % 200 == 0:\n",
    "                print(\n",
    "                    \"Episode: {:6}, Total_reward: {:7}, eps: {:6.4}, Total_reward_eval: {:7}\".format(\n",
    "                        i, total_reward, self.eps, self.eval())\n",
    "                )\n",
    "                self.eval()\n",
    "    \n",
    "    def eval(self, render=True):\n",
    "        state, _ = self.env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in count():\n",
    "            action = self.policy.get_action(state)\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            if render:\n",
    "                img = self.env.render()\n",
    "                \n",
    "            total_reward += reward\n",
    "            if terminated or truncated: break\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82d0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "q_net = DQN(4, 2)\n",
    "q_net.to(device)\n",
    "trainer = DQNTrainer(q_net, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3653caa",
   "metadata": {},
   "source": [
    "**Train by running 1500 episodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d75bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a8c2e",
   "metadata": {},
   "source": [
    "**Save the deep q-network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427538aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net.save(\"cartpole_1500.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0783b2a",
   "metadata": {},
   "source": [
    "**Test the performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e415c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(policy, env):\n",
    "    state, _ = env.reset()\n",
    "    for _ in count():\n",
    "        action = policy.get_action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            env.close()\n",
    "            break\n",
    "\n",
    "q_net.load(\"cartpole_1500.pth\")\n",
    "policy = EpsGreedy(q_net)\n",
    "\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "test(policy, test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f10470",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGC_ML",
   "language": "python",
   "name": "pgc_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
