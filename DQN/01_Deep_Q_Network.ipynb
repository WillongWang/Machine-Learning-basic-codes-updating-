{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596963ce",
   "metadata": {},
   "source": [
    "# Deep Q Network (same as DQN/dqn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a99e1a",
   "metadata": {},
   "source": [
    "What we need are:\n",
    "1. $Q_\\theta(s,a)$ to tell how good it is to take action $a$ given state $s$, parameterized by $\\theta$\n",
    "2. A policy $\\pi_\\theta(s)$ utilize the $Q_\\theta(s,a)$, which is $\\pi_\\theta(s)=\\arg\\max_a Q_\\theta(s,a)$, but also be able to explore different actions to find better one.\n",
    "3. Temporal difference $\\delta(s_t, a_t, s_{t+1}, r_{t+1})=Q_\\theta(s_t, a_t) - (r_{t+1} + \\max_{a_{t+1}}\\gamma Q_\\theta(s_{t+1}, a_{t+1}))$\n",
    "4. A replay buffer to store tranistion $(s_t, a_t, s_{t+1}, r_{t+1})$ to improve sample efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9dc08",
   "metadata": {},
   "source": [
    "### Define the Deep Q Network\n",
    "What we need is $Q_\\theta:S\\times A\\rightarrow R$.\n",
    "\n",
    "For a discrete action space, we can instead define a function of type $S\\rightarrow R^{|A|}$. We also need $V_\\theta:S\\rightarrow R$ defined as $V_\\theta(s) =\\max_a Q_\\theta(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c307f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, dim_state, n_actions):\n",
    "        super().__init__()\n",
    "        self.dim_state = dim_state\n",
    "        self.n_actions = n_actions\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim_state, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_actions)\n",
    "        )\n",
    "        self.forward = self.net.forward\n",
    "    \n",
    "    def q_vals(self, states, actions):\n",
    "        #Q(s,a)\n",
    "        return self.forward(states).gather(\n",
    "            1, actions.reshape(-1, 1).to(torch.int64)\n",
    "        ).reshape(-1)\n",
    "    \n",
    "    def v_vals(self, states):\n",
    "        #max_a Q(s,a)\n",
    "        return self.forward(states).amax(-1)\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.net.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.net.load_state_dict(torch.load(path))\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.net.parameters()).device\n",
    "\n",
    "    def get_compat(self, x):\n",
    "        x = np.stack(x)\n",
    "        x = torch.tensor(x, device=self.device)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6742109",
   "metadata": {},
   "source": [
    "### Define the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c7a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *arg):\n",
    "        self.deque.append(Transition(*arg))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.deque)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a3f99",
   "metadata": {},
   "source": [
    "### Define the Policy\n",
    "Use $\\epsilon$-greedy as exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0074f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class EpsGreedy:\n",
    "    def __init__(self, q_net):\n",
    "        self.q_value_net = q_net\n",
    "    \n",
    "    def get_action(self, state, mode=\"eval\", eps=0.1):\n",
    "        if mode == \"train\" and random.random() < eps:\n",
    "            return random.randint(0, self.q_value_net.n_actions-1)\n",
    "        else:\n",
    "            state = self.q_value_net.get_compat(state)\n",
    "            return self.q_value_net(state).argmax().item()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b5ec1",
   "metadata": {},
   "source": [
    "### Training\n",
    "To stablize training, use a target net, parametrized by $\\theta'$, to give value of the next state $s_{t+1}$.\n",
    "\n",
    "$\\delta(s_t, a_t, s_{t+1}, r_{t+1})=Q_\\theta(s_t, a_t) - (r_{t+1} + \\max_{a_{t+1}}\\gamma Q_{\\theta'}(s_{t+1}, a_{t+1}))$\n",
    "\n",
    "$\\theta'$ would be synchronized with $\\theta$ every a certain number of update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23037ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from itertools import count\n",
    "from IPython.display import display\n",
    "\n",
    "class DQNTrainer:\n",
    "    def __init__(self, q_net, env,\n",
    "                 lr=1e-4, gamma=0.99, replay_buf_size=10000,\n",
    "                 eps_start=0.5, eps_end=0.01, eps_decay=1e4,\n",
    "                 update_thres=500):\n",
    "        \n",
    "        self.q_net = q_net\n",
    "        self.env = env\n",
    "        self.opt = torch.optim.RMSprop(\n",
    "            self.q_net.parameters(),\n",
    "            lr=lr\n",
    "        )\n",
    "        self.gamma=gamma\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.policy = EpsGreedy(q_net)\n",
    "        self.replay_buf = ReplayBuffer(replay_buf_size)\n",
    "        self.target_net = copy.deepcopy(self.q_net)\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        \n",
    "        self.update_thres=update_thres\n",
    "        \n",
    "    @property\n",
    "    def eps(self):\n",
    "        ratio = np.exp(-self.counter/self.eps_decay)\n",
    "        eps = self.eps_end + (self.eps_start - self.eps_end)*ratio\n",
    "        return eps\n",
    "           \n",
    "    def update(self, batch_size):\n",
    "        if batch_size > len(self.replay_buf):\n",
    "            return\n",
    "        batch = self.replay_buf.sample(batch_size)\n",
    "        #check next_state to tell if is final\n",
    "        non_final_mask = [*map(lambda trans: trans.next_state is not None, batch)]\n",
    "        state, action, next_state, reward = zip(*batch)\n",
    "        next_state = [*filter(lambda x: x is not None, next_state)]\n",
    "\n",
    "        batch = Transition(*map(self.q_net.get_compat , [state, action, next_state, reward]))\n",
    "        #Q^(s,a)\n",
    "        state_action_vals = self.q_net.q_vals(batch.state, batch.action)\n",
    "        #V(s')\n",
    "        next_state_vals = torch.zeros_like(state_action_vals)\n",
    "        with torch.no_grad():\n",
    "            next_state_vals[non_final_mask] = self.target_net.v_vals(batch.next_state)\n",
    "        #Q(s,a)=r + gamma*V(s')\n",
    "        expected_state_action_vals = batch.reward + self.gamma * next_state_vals\n",
    "        \n",
    "        #compute loss & update\n",
    "        loss = self.criterion(state_action_vals, expected_state_action_vals)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        for para in self.q_net.parameters():\n",
    "            para.grad.data.clamp_(-1, 1)\n",
    "        self.opt.step()\n",
    "    \n",
    "    def train(self, n_episodes):\n",
    "        for i in range(n_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            for _ in count():\n",
    "                self.counter += 1\n",
    "                action = self.policy.get_action(state, \"train\", self.eps)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                \n",
    "                total_reward += reward\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                \n",
    "                self.replay_buf.push(state, action, next_state, reward)\n",
    "                \n",
    "                state = next_state\n",
    "                self.update(512)\n",
    "                if self.counter % self.update_thres == 0:\n",
    "                    self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "                \n",
    "                if terminated or truncated:break\n",
    "                    \n",
    "            if i % 200 == 0:\n",
    "                print(\n",
    "                    \"Episode: {:6}, Total_reward: {:7}, eps: {:6.4}, Total_reward_eval: {:7}\".format(\n",
    "                        i, total_reward, self.eps, self.eval())\n",
    "                )\n",
    "\n",
    "            if i == 100:\n",
    "                self.q_net.save(\"cartpole_100.pth\")\n",
    "                \n",
    "            if i == 1000:\n",
    "                self.q_net.save(\"cartpole_1000.pth\")\n",
    "                \n",
    "    \n",
    "    def eval(self, render=True):\n",
    "        state, _ = self.env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in count():\n",
    "            action = self.policy.get_action(state)\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            if render:\n",
    "                img = self.env.render()\n",
    "                \n",
    "            total_reward += reward\n",
    "            if terminated or truncated: break\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82d0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "q_net = DQN(4, 2)\n",
    "q_net.to(device)\n",
    "trainer = DQNTrainer(q_net, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3653caa",
   "metadata": {},
   "source": [
    "**Train by running 5000 episodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d75bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:      0, Total_reward:     9.0, eps: 0.4996, Total_reward_eval:    11.0\n",
      "Episode:    200, Total_reward:    11.0, eps: 0.3883, Total_reward_eval:    10.0\n",
      "Episode:    400, Total_reward:    11.0, eps: 0.3063, Total_reward_eval:     9.0\n",
      "Episode:    600, Total_reward:    10.0, eps:  0.246, Total_reward_eval:     9.0\n",
      "Episode:    800, Total_reward:    11.0, eps: 0.1944, Total_reward_eval:    12.0\n",
      "Episode:   1000, Total_reward:    67.0, eps: 0.09036, Total_reward_eval:    59.0\n",
      "Episode:   1200, Total_reward:   123.0, eps: 0.02357, Total_reward_eval:   100.0\n",
      "Episode:   1400, Total_reward:   226.0, eps: 0.01036, Total_reward_eval:   209.0\n",
      "Episode:   1600, Total_reward:   500.0, eps:   0.01, Total_reward_eval:   500.0\n",
      "Episode:   1800, Total_reward:   500.0, eps:   0.01, Total_reward_eval:   500.0\n",
      "Episode:   2000, Total_reward:   500.0, eps:   0.01, Total_reward_eval:   500.0\n",
      "Episode:   2200, Total_reward:   279.0, eps:   0.01, Total_reward_eval:   250.0\n",
      "Episode:   2400, Total_reward:    12.0, eps:   0.01, Total_reward_eval:    12.0\n",
      "Episode:   2600, Total_reward:    45.0, eps:   0.01, Total_reward_eval:    26.0\n",
      "Episode:   2800, Total_reward:    10.0, eps:   0.01, Total_reward_eval:    67.0\n",
      "Episode:   3000, Total_reward:    91.0, eps:   0.01, Total_reward_eval:    95.0\n",
      "Episode:   3200, Total_reward:    21.0, eps:   0.01, Total_reward_eval:    33.0\n",
      "Episode:   3400, Total_reward:    83.0, eps:   0.01, Total_reward_eval:    77.0\n",
      "Episode:   3600, Total_reward:    71.0, eps:   0.01, Total_reward_eval:    67.0\n",
      "Episode:   3800, Total_reward:    62.0, eps:   0.01, Total_reward_eval:    57.0\n",
      "Episode:   4000, Total_reward:    84.0, eps:   0.01, Total_reward_eval:    81.0\n",
      "Episode:   4200, Total_reward:   111.0, eps:   0.01, Total_reward_eval:   101.0\n",
      "Episode:   4400, Total_reward:    95.0, eps:   0.01, Total_reward_eval:    97.0\n",
      "Episode:   4600, Total_reward:   107.0, eps:   0.01, Total_reward_eval:   100.0\n",
      "Episode:   4800, Total_reward:   139.0, eps:   0.01, Total_reward_eval:   131.0\n"
     ]
    }
   ],
   "source": [
    "trainer.train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a8c2e",
   "metadata": {},
   "source": [
    "**Save the deep q-network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427538aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net.save(\"cartpole_5000.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0783b2a",
   "metadata": {},
   "source": [
    "**Test the performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e415c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2l/60vq7_hn4rv7mkg13339zghc0000gn/T/ipykernel_9128/2061914679.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "def test(policy, env):\n",
    "    state, _ = env.reset()\n",
    "    for _ in count():\n",
    "        action = policy.get_action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            env.close()\n",
    "            break\n",
    "#q_net.load(\"cartpole_100.pth\")\n",
    "#q_net.load(\"cartpole_1000.pth\")\n",
    "q_net.load(\"cartpole_5000.pth\")\n",
    "policy = EpsGreedy(q_net)\n",
    "\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "test(policy, test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f10470",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3a046-2d85-42e2-8a98-300e93557c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
